{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20c9e2f1",
   "metadata": {},
   "source": [
    "# Direct minimisation\n",
    "\n",
    "In this section we return our attention to the DFT minimisation problem\n",
    "$$ \\min_{P\\in\\mathcal{P}} \\mathcal{E}(P). $$\n",
    "Instead of solving the SCF problem, i.e. focusing on satisfying its first-order optimality condition, a mathematically more natural approach to directly develop methods to solve this minimisation problem. This is referred to as **direct minimisation** (DM).\n",
    "\n",
    "For simplicity we restrict the discussion of DM to the case of infinite $\\beta$ or zero temperature, where all occupations $f_i$ are either $0$ or $1$.\n",
    "In this case a reformulation of the minimisation problem in terms of the orbitals is trivial, since \n",
    "$$ P = \\sum_i^{N} |\\phi_i\\rangle \\langle \\phi_i|. $$\n",
    "where $N$ is the number of electrons. \n",
    "Therefore one may alternatively solve \n",
    "$$ \\displaystyle \\text{min}_{\\{\\psi_i\\}} \\mathcal{E}_\\text{DFT}(\\{\\psi_i\\}) $$\n",
    "where the $\\{\\psi_i\\}$ are constrained to be orthonormal orbitals. For non-zero temperature other occupations are possible, which overall complicates the picture and is not discussed here.\n",
    "\n",
    "\n",
    "Due to the orthogonality constraints on the orbitals $\\{\\psi_i\\}$,\n",
    "  the set of admissible orbitals does not form a vector space,\n",
    "  but much rather the unknowns of $\\{\\psi_i\\}$ belong to a Stiefel manifold.\n",
    "  This needs to be taken into account\n",
    "  (via appropriate projections) in order to get the correct minimum\n",
    "in a minimisation procedure. For more details, see for example [A. Edelman, T. Arias, S. Smith *SIAM J. Mat. Anal. Appl.* **20**, 303 (1998) DOI 10.1137/s0895479895290954](http://dx.doi.org/10.1137/s0895479895290954).\n",
    "\n",
    "The gradient of $\\mathcal{E}_0$ wrt. orbitals is easily obtained via the chain rule from our previous developments:\n",
    "  $$ \\left\\langle \\phi \\middle| \\nabla_{\\psi_i} \\mathcal{E}_0\\left(\\sum_i |\\psi_i\\rangle \\langle \\psi_i |\\right) \\right\\rangle = \\langle \\phi | \\nabla_P \\mathcal{E}_0(P) \\ \\psi_i\\rangle \n",
    "  + \\langle \\psi_i | \\nabla_P \\mathcal{E}_0(P) \\ \\phi \\rangle\n",
    "  = 2 \\langle \\phi | H_\\text{KS} \\psi_i \\rangle$$\n",
    "\n",
    "To show this in practice we proweed to a simple implementation using `DFTK.jl` for the energy functional of the density-functional theory problem and `Optim.jl` to do the minimisation on the Stiefal manifold. To keep the implementation simple we further restrict ourselves to a single $k$-Point. Going beyond that requires a little more bookkeeping\n",
    "  (see the [DFTK implementation](https://github.com/JuliaMolSim/DFTK.jl/blob/master/src/scf/direct_minimization.jl))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26a4024e",
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching model_LDA(::Matrix{Float64}, ::Vector{Pair{ElementPsp, Vector{Vector{Float64}}}})\n\u001b[0mClosest candidates are:\n\u001b[0m  model_LDA(::AbstractMatrix, \u001b[91m::Vector{<:DFTK.Element}\u001b[39m, \u001b[91m::Vector{<:AbstractVector}\u001b[39m; kwargs...) at ~/.julia/packages/DFTK/rFq5C/src/standard_models.jl:51\n\u001b[0m  model_LDA(\u001b[91m::AtomsBase.AbstractSystem\u001b[39m, ::Any...; kwargs...) at ~/.julia/packages/DFTK/rFq5C/src/external/atomsbase.jl:94",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching model_LDA(::Matrix{Float64}, ::Vector{Pair{ElementPsp, Vector{Vector{Float64}}}})\n\u001b[0mClosest candidates are:\n\u001b[0m  model_LDA(::AbstractMatrix, \u001b[91m::Vector{<:DFTK.Element}\u001b[39m, \u001b[91m::Vector{<:AbstractVector}\u001b[39m; kwargs...) at ~/.julia/packages/DFTK/rFq5C/src/standard_models.jl:51\n\u001b[0m  model_LDA(\u001b[91m::AtomsBase.AbstractSystem\u001b[39m, ::Any...; kwargs...) at ~/.julia/packages/DFTK/rFq5C/src/external/atomsbase.jl:94",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[1]:12",
      " [2] eval",
      "   @ ./boot.jl:373 [inlined]",
      " [3] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)",
      "   @ Base ./loading.jl:1196"
     ]
    }
   ],
   "source": [
    "using DFTK\n",
    "using Optim\n",
    "using LineSearches\n",
    "\n",
    "# Standard silicon setup\n",
    "a = 10.26\n",
    "lattice = a / 2 * [[0 1 1.];\n",
    "                   [1 0 1.];\n",
    "                   [1 1 0.]]\n",
    "Si = ElementPsp(:Si, psp=load_psp(\"hgh/lda/si-q4\"))\n",
    "atoms = [Si => [ones(3)/8, -ones(3)/8]]\n",
    "model  = model_LDA(lattice, atoms)\n",
    "basis  = PlaneWaveBasis(model; Ecut=10, kgrid=[1, 1, 1]);\n",
    "\n",
    "# One unit cell has 2 Silicon atoms.\n",
    "# In the model we use (where only valence electrons are explictly treated)\n",
    "# this makes 8 electrons, which requires 4 bands with 2 electrons each:\n",
    "occupation = [2.0, 2.0, 2.0, 2.0]\n",
    "\n",
    "# We specify a random initial guess for the 4 orbitals:\n",
    "n_G = length(G_vectors(only(basis.kpoints)))\n",
    "ψ0 = Matrix(qr(randn(ComplexF64, n_G, 4)).Q);\n",
    "\n",
    "# Function to compute energies and gradients\n",
    "function fg!(E, G, ψ)\n",
    "    ρ = compute_density(basis, [ψ], [occupation])\n",
    "    energies, H = energy_hamiltonian(basis, [ψ], [occupation]; ρ=ρ)\n",
    "\n",
    "    if G !== nothing\n",
    "        # Optim expects the gradient in G\n",
    "        occupation_ψ = 2.0  # Hard-coded occupation of orbital ψ \n",
    "        G .= 2 * occupation_ψ * (H.blocks[1] * ψ)\n",
    "    end\n",
    "    energies.total\n",
    "end\n",
    "\n",
    "# Select a quasi-Newton algorithm with backtracking linesearches\n",
    "# to avoid to many costly gradient evaluations\n",
    "algorithm = Optim.LBFGS(manifold=Optim.Stiefel(),\n",
    "                        linesearch=LineSearches.BackTracking())\n",
    "\n",
    "# Set some convergence options in Optim:\n",
    "options = Optim.Options(; allow_f_increases=true, show_trace=true, x_tol=1e-6)\n",
    "\n",
    "# Run the direct minimisation\n",
    "res = Optim.optimize(Optim.only_fg!(fg!), ψ0, algorithm, options)\n",
    "\n",
    "@show res.minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae673e13",
   "metadata": {},
   "source": [
    "Finally let us note that DM in combination with metals (also known as *ensemble density-functional theory*) is tricky. In this case there is no band gap and thus finite $\\beta$ is usually taken. Furthermore there are possible degeneracies of the orbitals at the Fermi level ($i = N$).\n",
    "  This problem manifests when computing the gradient of $\\mathcal{E}_\\text{DFT}$\n",
    "  in such a setting, where thus special care is needed to not run into\n",
    "  \"division by zero\" issues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc250040",
   "metadata": {},
   "source": [
    "## Comparing SCF and DM methods\n",
    "\n",
    "In this section we want to elaborate on the relationship between self-consistent-field and direct minimisation methods. For this we return to the finite-temperature setting in the grand-canonical ensemble and use the formulation in terms of density matrices\n",
    "$$ \\min_{P\\in\\mathcal{P}} \\mathcal{E}(P). $$\n",
    "where \n",
    "$$ \\mathcal{E}(P) = \\mathcal{E}_0(P) - \\frac{1}{\\beta} \\mathrm{Tr}[s(P)] - \\mu \\mathrm{Tr}(P). $$\n",
    "and\n",
    "$$\\begin{align*}\n",
    "    \\mathcal{H} = \\left\\{H \\in \\mathbb{C}^{N_b \\times N_b}, H^\\dagger = H \\right\\}.\n",
    "    \\mathcal{P} = \\left\\{P \\in \\mathcal{H}, 0 < P < 1 \\right\\}.\n",
    "\\end{align*}$$\n",
    "\n",
    "The gradient of $\\mathcal{E}_0$ we already identified as $ H_\\text{KS}(P) = \\nabla \\mathcal{E}_0(P)$. In analogy to the second derivative of $g(\\rho)$ we denote the hessian of $\\mathcal{E}_0$ as\n",
    "$$ \\underline{K}(P) = \\underline{d^2 \\mathcal{E}_0}(P) = \\underline{d \\nabla \\mathcal{E}_0}(P), $$\n",
    "where we use an underline denote this object as a\n",
    "**super-operators**, i.e. operators from $\\mathcal{H}$ to $\\mathcal{H}$.\n",
    "Following from the analogy we also refer to $\\underline{K}$ as the \"four-point\" kernel.\n",
    "\n",
    "Similarly we can obtain the four-point analogon to $\\chi_0 = D'$ as the derivative $\\underline{\\chi_0} = d f_\\text{FD}$ computed as\n",
    "$$\\begin{align*}\n",
    "      &\\underline{\\chi_{0}}\\left(\\sum_{i=1}^{N_{\\rm b}} \\varepsilon_{i} |\\phi_{i}\\rangle\\langle \\phi_{i}|\\right) \\cdot \\delta H = \\sum_{i=1}^{N_{\\rm b}}\\sum_{j=1}^{N_{\\rm b}} \\frac{f_\\text{FD}(\\varepsilon_{i}) - f_\\text{FD}(\\varepsilon_{j})}{\\varepsilon_{i}-\\varepsilon_{j}}\\langle  \\phi_{i}, \\delta H \\phi_{j} \\rangle |\\phi_{i}\\rangle\\langle \\phi_{j}|\n",
    "\\end{align*}$$\n",
    "(for details see [M. Herbst, A. Levitt *J. Comput. Phys.* **459**, 111127 (2022)](http://dx.doi.org/10.1016/j.jcp.2022.111127)). Comparing with the Adler-Wiser formula makes the analogy to $\\chi_0$ particularly apparent.\n",
    "\n",
    "\n",
    "Further we will need the gradient and Hessian of the free energy $\\mathcal{E}$ for our analysis.\n",
    "We recall the gradient of $\\mathcal{E}$ as\n",
    "$ \\nabla \\mathcal{E}(P) = H_\\text{KS}(P) - \\mu - \\frac1{\\beta} s'(P). $\n",
    "To obtain the Hessian we differentiate once more\n",
    "  $$\n",
    "  \\underline{d^2 \\mathcal{E}}(P) \\cdot \\delta P\n",
    "  = \\underline{K}(H_\\text{KS}(P)) \\cdot \\delta P\n",
    "  - \\frac{1}{\\beta}\n",
    "  \\sum_{i=1}^{N_b} \\sum_{j=1}^{N_b}\n",
    "  \\frac{s'(p_i) - s'(p_j)}{p_i - p_j}\n",
    "  |\\phi_i \\rangle \\langle \\phi_i | \\delta P \\phi_j\\rangle\n",
    "  \\langle \\phi_j |\n",
    "  .$$\n",
    "Defining\n",
    "  $$\n",
    "  \\underline{\\Omega}\\left(\n",
    "      \\sum_{i=1}^{N_b} \\varepsilon_i \\phi_i \\rangle \\langle \\phi_i |\n",
    "  \\right) \\cdot \\delta P\n",
    "  = - \\sum_{i=1}^{N_b} \\sum_{j=1}^{N_b}\n",
    "  \\frac{\\varepsilon_i - \\varepsilon_j}{f_\\text{FD}(\\varepsilon_i) - f_\\text{FD}(\\varepsilon_j)}\n",
    "  |\\phi_i \\rangle \\langle \\phi_i | \\delta P \\phi_j\\rangle\n",
    "  \\langle \\phi_j |\n",
    "  $$\n",
    "  we write this more compactly as\n",
    "  $$\n",
    "  \\underline{d^2 \\mathcal{E}}(P)\n",
    "  = \\underline{K}(H_\\text{KS}(P)) + \\underline{\\Omega}(f_\\text{FD}^{-1}(P)).\n",
    "  $$\n",
    "\n",
    "Comparing the expressions for $\\underline{\\chi_0}$ and $\\underline{\\Omega}$ we furthermore note for later convenience that\n",
    "$$ \\underline{\\Omega} = - \\underline{\\chi_0}^{-1}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fcc4a4",
   "metadata": {},
   "source": [
    "### Direct minimisation in density-matrix formalism\n",
    "\n",
    "As an example for a direct minimisation approach we will consider **damped gradient descent**\n",
    "$$ P_{n+1} = P_n - \\alpha \\nabla \\mathcal{E}(P_n). $$\n",
    "\n",
    "If $\\alpha$ is chosen small enough, this iteration converges to a local minimiser $P_\\ast$, which we assume to be non-degenerate. To understand the convergence we denote the error in each iteration as $E_n = P_n - P_\\ast$ and write\n",
    "$$\\begin{align*}\n",
    "E_{n+1} &= E_n - \\alpha \\nabla \\mathcal{E}(P_\\ast + E_n) \\\\\n",
    "&= E_n - \\alpha [\\nabla \\mathcal{E}(P_\\ast) + \\underline{d^2 \\mathcal{E}}(P_\\ast) \\cdot E_n ] \\\\\n",
    "&= \\left[1 - \\alpha (\\underline{K}_\\ast + \\underline{\\Omega}_\\ast)\\right] E_n,\n",
    "\\end{align*}$$\n",
    "where $|_\\ast$ denotes that these quantities are evaluated at the fixed-point density matrix $P_\\ast$ or Hamiltonian $H(P_\\ast)$.\n",
    "\n",
    "Similar to our discussion in the previous notebook the asymptotic rate of convergence is related to the eigenvalues of\n",
    "$1 - \\alpha J_\\text{grad}$\n",
    "$$J_\\text{grad} = \\underline{K}_\\ast + \\underline{\\Omega}_\\ast.$$\n",
    "In particular with $\\alpha$ chosen optimally it is the spectral condition number $\\kappa = \\frac{\\lambda_\\text{max}}{\\lambda_\\text{min}}$ which dictates the rate of convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae407f63",
   "metadata": {},
   "source": [
    "### SCF in density-matrix formalism\n",
    "\n",
    "The fixed-point problem\n",
    "$$ P = f_\\text{FD}(H_\\text{KS}(P)) $$\n",
    "in the density-matrix formalism of DFT is fully analogous\n",
    "to the problem\n",
    "$$ \\rho = D(V(\\rho))$$\n",
    "underlying the density-mixing SCF algorithm we discussed in the previous notebook. Using the appropriate four-point analoges to $\\chi_0$ and $K$ our considerations wrt. convergence of density mixing can therefore be directly adapted to a density-matrix mixing algorithm.\n",
    "\n",
    "In particular the convergence of the damped iterations\n",
    "$$ P_{n+1} = P_n + \\alpha [f_\\text{FD}(H_\\text{KS}(P_n)) - P_n]$$\n",
    "are characterised by the eigenvalues of $1 - \\alpha J_\\text{SCF}$\n",
    "with\n",
    "$$ J_\\text{SCF} = 1 - \\left. \\underline{\\chi_0}\\right|_\\ast \\underline{K}_\\ast = 1 + \\underline{\\Omega}_\\ast^{-1} \\underline{K}_\\ast, $$ \n",
    "where $|_\\ast$ denotes that these quantities are evaluated at the fixed-point density matrix $P_\\ast$ or Hamiltonian $H(P_\\ast)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd1a540",
   "metadata": {},
   "source": [
    "### Comparison of convergence analysis\n",
    "\n",
    "Both methods have a very similar structure, differing only in the matrix $J$ of the Jacobian $1 - \\alpha J$. Most notably is the appearance of $\\underline{\\Omega}_\\ast$ as a summand in $J_grad$ for gradient descent and as an *inverse* in $J_\\text{SCF}$.\n",
    "\n",
    "To keep the discussion here simple, we will only give a handwavy argument based on the extremal eigenvalues of $\\underline{\\Omega}_\\ast$. For a more detailed discussion for the case of insulators at zero temperature see [E. Cances, G. Kemlin, A. Levitt *SIAM J. Mat. Anal. Appl.* **42**, 243 (2021) DOI 10.1137/20m1332864](http://dx.doi.org/10.1137/20m1332864).\n",
    "\n",
    "The largest eigenvalue of $\\underline{\\Omega}_\\ast$ depends on $\\varepsilon_{N_b} - \\varepsilon_1$, i.e. the difference between the largest and smallest eigenvalue of the Kohn-Sham Hamiltonian, which becomes larger and larger as the discretisation is improved. While this is not a big problem for SCF methods, this can become an issue for gradient descent in theory. However, in practice it can be cured by appropriate preconditioning.\n",
    "\n",
    "If the smallest eigenvalue of $\\underline{\\Omega}_\\ast$ is close to zero, this gives rise to a large eigenvalue in $J_\\text{SCF}$, while the smallest eigenvalue of $J_\\text{grad}$ stays bounded due to the $\\underline{K}$ term. While small eigenvalues in $\\underline{\\Omega}_\\ast$ are thus a problem for SCF methods they are not for direct minimisation methods.\n",
    "\n",
    "How can such small gaps arise? Assume one only uses a very small temperature, such that $\\beta$ is large and $f_\\text{FD}$ is pretty much a step function. In this case a good approximate lower bound to the smallest eigenvalue of $\\underline{\\Omega}_\\ast$ is the gap $\\nu = \\varepsilon_a - \\varepsilon_i$, where $a$ denotes the first orbital above the Fermi level ($\\varepsilon_a > \\mu$) and $i$ the last orbital below the Fermi level ($\\varepsilon_i < \\mu$). A closing gap $\\nu$ thus causes the smallest eigenvalue of $\\underline{\\Omega}_\\ast$ to decrease as well.\n",
    "If one wants to stick with SCF methods the remedy is increase the temperature (use a smaller $\\beta$). For metallic systems, where the gap is zero, this always has to be done. For metals with $\\nu=0$ the smallest eigenvalue of $\\underline{\\Omega}_\\ast$ is bounded from below by $-1 / f'_\\text{FD}(\\mu) = 4/\\beta$, such that a large enough temperature can ensure convergence of the SCF.\n",
    "\n",
    "Whether SCF or direct minimisation should be employed in practice overall depends not only on the considerations of the convergence rate outlined here, but also questions related to the computational cost of each step, the effectiveness of convergence acceleration schemes and the robustness of iterations needs to be considered. In any case the mathematical anlysis outlined here, suggests that direct minimisation methods should be beneficial in particular for modelling insulators with smaller gaps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
